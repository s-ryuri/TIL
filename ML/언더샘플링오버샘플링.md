label이 불균형한 분포를 가진 데이터세트는 이상 레이블을 가지는 데이터 수가 정상 레이블을 가진 데이터 수에 비해 너무 적기 때문에 예측 성능의 문제가 발생할 수 있다.   
왜냐 이상 레이블 데이터 수는 적기에 다양한 유형을 학습하지 못하고 정상 레이블 데이터 수는 많기에 정상 레이블로 치우친 학습을 수행해 제대로 된 이상 데이터 검출이 어렵기 때문이다.

지도학습에서 불균형한 레이블 값 분포로 인한 문제점을 해결하기 위해서는 적절한 학습 데이터를 확보하는 방안이 필요한데 대표적인 것인 oversampling(오버 샘플링)과 언더 샘플링(Undersampling) 방법이 있다.   
보통 오버 샘플링 방식이 예측 성능상 더 유리한 경우가 많아서 주로 사용된다.

# 언더 샘플링

![image](https://user-images.githubusercontent.com/66999675/132154801-fc3f0c8c-11f6-4482-9c7b-fe61771b4b7e.png)  

언더 샘플링은 많은 데이터 세트를 적은 데이터 세트 수준으로 감소시키는 방식이다.   
즉 정상 레이블을 가진 데이터가 5만 건 이상 레이블을 가진 데이터가 500건 있으면 정상 레이블 데이터를 500건으로 줄이는 방식이다.  
이 방식은 과도하게 정상 레이블로 학습/예측하는 부작용을 개선할 수 있지만, 너무 많은 정상 레이블 데이터를 감소시키기에 정상 레이블의 경우에 오히려 제대로 된 학습을 수행할 수 없다는 단점이 있기에 잘 사용하지 않는다.  

![image](https://user-images.githubusercontent.com/66999675/132154849-c9c333d3-4deb-4b67-a2ec-c535bcf7ca8d.png)  

오버 샘플링은 적은 데이터 세트를 증식하여 학습을 위한 충분한 데이터를 확보하는 방법이다. 동일한 데이터를 단순하게 늘리는 방식은 overfitting이 되기에 의미가 없다.   
원본 데이터 feature 값들을 아주 약간만 변경해서 늘린다. 대표적인 방식으로 SMOTE(Synthetic Minority Over-sampling Technique) 방법이 있다.  
SMOTE는 적은 데이터 세트에 있는 개별 데이터들의 K Nearest Neigthbor을 찾아서 이 데이터와 K 개 이웃들의 차이를 일정 값으로 만들어서 기존 데이터와 약간 차이가 나는 새로운 데이터들을 생성하는 방식이다.
